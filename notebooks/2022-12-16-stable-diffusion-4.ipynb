{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNG3Y4A_mMl2"
   },
   "source": [
    "# Stable Diffusion from Begginer to Master (4) Text Encoder and Tokenizer\n",
    "- date: \"2022-12-16\"\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [Deep Learning, Python]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fPkiI8gVLSb"
   },
   "source": [
    "In this tutorial we'll take a deeper look into the text processing components of stable diffusion - the `TextEncoder` and the `Tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4dSGPJrVX7i"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dArFcurSPP0j"
   },
   "outputs": [],
   "source": [
    "!pip install -Uqq diffusers transformers ftfy accelerate bitsandbytes\n",
    "#!pip install -Uqq triton xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lpP-uG8WQ5G"
   },
   "source": [
    "# Pipeline\n",
    "\n",
    "We use the same stable diffusion pipeline from last tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "0addbc70710149369d358521762b62dc",
      "87395cfdfbfa4267850ed52996c59e5b",
      "c95d0f37320a4c7c816b98d7f6d76e4d",
      "077d045135d24c9599d5e0872d9ca9cf",
      "8757169d3fcb4abb903cf552f14afba7",
      "35e204c5e19048a6bfb6409ee98b38d5",
      "ffdb91d3e58e487c93d9c4b705dfe8af",
      "b7ef24137af24a699a21a0428283c7a4",
      "9c024792d7384dee8a20fbd46f734809",
      "0a5311d5e8e740ac93f5bd05839fd4e0",
      "05be7abbc0c647ee917fd88f37ef974c"
     ]
    },
    "id": "qG1wgu8kWUdW",
    "outputId": "269aee3e-adcc-4735-c0f1-fbda84876698"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0addbc70710149369d358521762b62dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pprint\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pipe = StableDiffusionPipeline.from_pretrained('stabilityai/stable-diffusion-2-base', revision=\"fp16\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3OyI0GzzMpT"
   },
   "source": [
    "Now we can access the text encoder and tokenizer by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXtph5QZyvos",
    "outputId": "4690539a-cf5e-4a66-bc52-eed3f287fd8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='/root/.cache/huggingface/diffusers/models--stabilityai--stable-diffusion-2-base/snapshots/1cb61502fc8b634cdb04e7cd69e06051a728bedf/tokenizer', vocab_size=49408, model_max_len=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '!'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDrSTEilzTSM"
   },
   "outputs": [],
   "source": [
    "pipe.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD9vkKhLzV9F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZhJz0eS1UkH"
   },
   "source": [
    "# Tokenizer\n",
    "\n",
    "The `Tokenizer` does two things:\n",
    "\n",
    "1. Breaks down a long text into \"tokens\" (the `tokenize` method).\n",
    "2. Converts tokens into a list of integer ids with value range `0 ~ vocabsize-1` , which are indices into an embedding matrix(the `convert_tokens_to_ids` method). This is essentially just a dictionary lookup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kk6Hqb4j8nkK",
    "outputId": "0651d889-0d28-4372-c2b7-cfbdc041ccf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a</w>', 'high', 'res</w>', 'photo</w>', 'of</w>', 'a</w>', 'woman</w>', 'wearing</w>', 'a</w>', 'red</w>', 'dress</w>']\n"
     ]
    }
   ],
   "source": [
    "tokens = pipe.tokenizer.tokenize(\"a highres photo of a woman wearing a red dress\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efuL8kdA8wr-"
   },
   "source": [
    "Tokens are not necessarily one-to-one with words - you can see from the above example that `highres` is broken into two words - `high` and `res</w>`. The symbol `</w>` inidicates the end of a word, so for example `of` and `of</w>` are two different tokens, the former meaning the `of` is in the middle of some other word. This clever and powerful idea allows us to process words that are not seen in the training data by breaking them into \"subwords\" that exist in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2vFu0se__LT"
   },
   "source": [
    "Now we can convert the tokens to integer ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gl9L1Fwz_2hi",
    "outputId": "9872d0ae-47b7-4956-c975-913e0f64fd61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[320, 1487, 934, 1125, 539, 320, 2308, 3309, 320, 736, 2595]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.convert_tokens_to_ids(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_qAYSI88ZQf"
   },
   "source": [
    "To allow batch processing, we always pad the ids to a fixed max length (77 in the stable diffusion case). If there are more than this number of tokens, the text gets truncated.\n",
    "\n",
    "To know where the paddings start and end, we also construct a mask that marks the text as 1s and paddings as 0s.\n",
    "\n",
    "To do all these processing in one go, we can use the `tokenizer()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJ4tt8qy1VgU",
    "outputId": "1c60d2aa-8f53-4f25-8cb0-1b1aaa67779b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1]]),\n",
      " 'input_ids': tensor([[49406,   320,  1487,   934,  1125,   539,   320,  2308,  3309,   320,\n",
      "           736,  2595, 49407,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n",
      "          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n",
      "          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n",
      "          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n",
      "          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n",
      "          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n",
      "          1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,  1070,\n",
      "          1070,  1070,  1070,  1070,  1070,  1070, 49407]])}\n"
     ]
    }
   ],
   "source": [
    "text_inputs = pipe.tokenizer(\n",
    "    [\"a highres photo of a woman wearing a red dress\",\n",
    "     ' '.join([\"a\"] + [\"very\"]*100 + [\"text\"])],\n",
    "    padding=\"max_length\",\n",
    "    max_length=pipe.tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\")\n",
    "pprint.pprint(text_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZEOGzpf5N5m"
   },
   "source": [
    "Comparing this result with the previous one, seems there are two addtional tokens `49406` and `49407` in the beginning and end of the integer ids list.\n",
    "\n",
    "If we want to debug the input ids, we can also use the tokenizer to decode them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "DvTG4dcT5aiH",
    "outputId": "c04ae3a6-02b5-48c6-85d3-5e8cd6040541"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|startoftext|>a highres photo of a woman wearing a red dress <|endoftext|>'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.decode(torch.tensor([49406,   320,  1487,   934,  1125,   539,   320,  2308,  3309,   320, 736,  2595, 49407]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNf6Owt4-iZA"
   },
   "source": [
    "The tokenizer conventionally adds `<|startoftext|>` and `<|endoftext|>` tokens to the sentence. These tokens help the model to learn the beginning and end of the sentence.\n",
    "\n",
    "Let's check how many tokens there are in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wmgskdrw_l-I",
    "outputId": "4423ed81-63c1-4429-8f3e-c081ac17c609"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49408"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adDiuRBMKYJD"
   },
   "source": [
    "What are the special tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_xi_WGNBfkJ",
    "outputId": "e8771805-6226-4a2e-df71-4ee37c9e044c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<|startoftext|>',\n",
       " 'eos_token': '<|endoftext|>',\n",
       " 'pad_token': '!',\n",
       " 'unk_token': '<|endoftext|>'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRXEfWD4Kakm"
   },
   "source": [
    "How does integer correspond to tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vsfx5IlXKG8Y",
    "outputId": "1777b5fb-3c76-4edb-c2a3-90dbb73b5f5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a</w>', 'photo</w>', '<|startoftext|>', '<|endoftext|>')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.decoder[320], pipe.tokenizer.decoder[1125], pipe.tokenizer.decoder[49406], pipe.tokenizer.decoder[49407]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BeOKNmhoKob7"
   },
   "source": [
    "How is an unknown word handled?\n",
    "\n",
    "I haven't found a way to get a out-of-vocabulary token, since the vocabulary contains all possible bytes, which serves as a fall back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Ga8dhgmLG_h",
    "outputId": "7c804f51-91dc-427f-85e0-ccbfeb8f6d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ä½',\n",
       " 'ł',\n",
       " 'å¥',\n",
       " '½</w>',\n",
       " ',</w>',\n",
       " 'ab',\n",
       " 'cd</w>',\n",
       " ',</w>',\n",
       " 'asdf',\n",
       " 'h',\n",
       " 'j',\n",
       " 'kl',\n",
       " 'mn</w>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.tokenize('你好, abcd, asdfhjklmn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdlf_V78m3P_"
   },
   "source": [
    "To see the mapping between integer indices and tokens, use the `get_vocab` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0JfBl2Y4K8PM",
    "outputId": "020c7517-16cc-4319-c204-678b67f12522"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [49406, 596, 4480, 267, 36857, 71, 73, 8498, 4057, 49407], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer('abcd, asdfhjklmn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3wTgEJfm1rk",
    "outputId": "b971f5e7-af8b-4dec-efdb-2f91d961ba17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftext|>',\n",
       " 'ab',\n",
       " 'cd</w>',\n",
       " ',</w>',\n",
       " 'asdf',\n",
       " 'h',\n",
       " 'j',\n",
       " 'kl',\n",
       " 'mn</w>',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token = {idx: token for token, idx in pipe.tokenizer.get_vocab().items() }\n",
    "[idx2token[i] for i in pipe.tokenizer('abcd, asdfhjklmn')['input_ids']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9muRVSHBLRA3"
   },
   "source": [
    "How are punctuations preprocessed?\n",
    "\n",
    "Nothing in particular - as seen in the above example, punctuations like commas usually have their own token (`,</w>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgFWJ3cB9Iww"
   },
   "source": [
    "To know more about tokenizers, check out the huggingface [tutorial](https://huggingface.co/course/chapter2/4?fw=pt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv3xTVMCMweT"
   },
   "source": [
    "# Text Encoder\n",
    "\n",
    "The text encoder is a [CLIPTextModel](https://github.com/huggingface/transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L765).\n",
    "\n",
    "It is a transfomer model that takes token ids from tokenizer as input and get an embedding of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyZ5oxSWM5sj",
    "outputId": "08f1c8eb-7a62-4e14-bb79-2173fe09fe82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'),\n",
      " 'input_ids': tensor([[49406,   320,  1125,   539,   320,  2368, 49407,     0,     0,     0,\n",
      "             0],\n",
      "        [49406,   320,  1125,   539,   320,  2308,  3309,   320,   736,  2595,\n",
      "         49407]], device='cuda:0')}\n",
      "{'last_hidden_state': tensor([[[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n",
      "         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n",
      "         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n",
      "         ...,\n",
      "         [-0.0199, -0.2195, -0.0608,  ...,  0.1279,  0.1672, -0.1105],\n",
      "         [-0.0690, -0.2585, -0.0515,  ...,  0.1525,  0.1367, -0.1448],\n",
      "         [-0.0992, -0.2791, -0.0477,  ...,  0.1680,  0.1204, -0.1660]],\n",
      "\n",
      "        [[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n",
      "         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n",
      "         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n",
      "         ...,\n",
      "         [ 0.2007,  0.2732, -0.4333,  ...,  1.0098, -1.6348,  0.8604],\n",
      "         [ 0.2339,  2.2188, -0.5488,  ...,  0.0466, -2.1484,  0.1888],\n",
      "         [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>),\n",
      " 'pooler_output': tensor([[ 0.3347, -0.0175,  1.0537,  ...,  0.6938, -0.4158, -1.3076],\n",
      "        [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<IndexBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "inputs = pipe.tokenizer([\"a photo of a cat\", \"a photo of a woman wearing a red dress\"],\n",
    "                        padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "pprint.pprint(inputs)\n",
    "outputs = pipe.text_encoder(**inputs)\n",
    "pprint.pprint(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrQEHZliP1q-"
   },
   "source": [
    "So by default the input is padded to the max token length of the batch of text.\n",
    "\n",
    "Does padding matter when calling text encoder? Let's see what happens if the inputs are padded to 77 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NAxreR-PgwG",
    "outputId": "70059cd1-d269-4d4c-aebe-704b53d7e518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0'),\n",
      " 'input_ids': tensor([[49406,   320,  1125,   539,   320,  2368, 49407,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [49406,   320,  1125,   539,   320,  2308,  3309,   320,   736,  2595,\n",
      "         49407,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0')}\n",
      "{'last_hidden_state': tensor([[[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n",
      "         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n",
      "         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n",
      "         ...,\n",
      "         [-0.0199, -0.2195, -0.0608,  ...,  0.1279,  0.1672, -0.1105],\n",
      "         [-0.0690, -0.2585, -0.0515,  ...,  0.1525,  0.1367, -0.1448],\n",
      "         [-0.0992, -0.2791, -0.0477,  ...,  0.1680,  0.1204, -0.1660]],\n",
      "\n",
      "        [[-0.3135, -0.4475, -0.0083,  ...,  0.2544, -0.0327, -0.2959],\n",
      "         [ 0.1987, -1.6914, -0.8955,  ...,  0.4661, -0.0961, -2.1465],\n",
      "         [ 1.0234, -0.7349, -2.5430,  ...,  0.8960, -0.0602, -1.0723],\n",
      "         ...,\n",
      "         [ 0.2007,  0.2732, -0.4333,  ...,  1.0098, -1.6348,  0.8604],\n",
      "         [ 0.2339,  2.2188, -0.5488,  ...,  0.0466, -2.1484,  0.1888],\n",
      "         [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>),\n",
      " 'pooler_output': tensor([[ 0.3347, -0.0175,  1.0537,  ...,  0.6938, -0.4158, -1.3076],\n",
      "        [-0.9775,  0.2269,  2.0020,  ...,  0.0747, -0.2448, -1.8760]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<IndexBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "inputs1 = pipe.tokenizer([\"a photo of a cat\", \"a photo of a woman wearing a red dress\"],\n",
    "                         padding=\"max_length\", max_length=pipe.tokenizer.model_max_length, return_tensors=\"pt\").to(\"cuda\")\n",
    "pprint.pprint(inputs1)\n",
    "outputs1 = pipe.text_encoder(**inputs)\n",
    "pprint.pprint(outputs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsH-Bk_2QdhE"
   },
   "source": [
    "We can check that padding does not affect encoder result at all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_L2OPt7sMzA0",
    "outputId": "bc3ae86f-ed87-44c9-8300-6a3919d5898f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(outputs.last_hidden_state, outputs1.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Liw69HBlrhv8"
   },
   "source": [
    "Now we can take a closer look at the text encoding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_UsV9-hdQNzu",
    "outputId": "892cb9ac-be4b-413a-e00f-29d80900793d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextTransformer(\n",
       "  (embeddings): CLIPTextEmbeddings(\n",
       "    (token_embedding): Embedding(49408, 1024)\n",
       "    (position_embedding): Embedding(77, 1024)\n",
       "  )\n",
       "  (encoder): CLIPEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (12): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (13): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (14): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (15): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (16): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (17): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (18): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (19): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (20): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (21): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (22): CLIPEncoderLayer(\n",
       "        (self_attn): CLIPAttention(\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): CLIPMLP(\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.text_encoder.text_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SccQZdsPspH9"
   },
   "source": [
    "The text model is a `CLIPTextTransformer`, described in detail in the [CLIP](https://openai.com/blog/clip/) paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "19jGisTOrEkH"
   },
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPTextTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4TdTg7SLrsTg"
   },
   "outputs": [],
   "source": [
    "CLIPTextTransformer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2uL3otC10bm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "q4dSGPJrVX7i"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05be7abbc0c647ee917fd88f37ef974c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "077d045135d24c9599d5e0872d9ca9cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9c024792d7384dee8a20fbd46f734809",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b7ef24137af24a699a21a0428283c7a4",
      "value": 12
     }
    },
    "0a5311d5e8e740ac93f5bd05839fd4e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0addbc70710149369d358521762b62dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c95d0f37320a4c7c816b98d7f6d76e4d",
       "IPY_MODEL_077d045135d24c9599d5e0872d9ca9cf",
       "IPY_MODEL_8757169d3fcb4abb903cf552f14afba7"
      ],
      "layout": "IPY_MODEL_87395cfdfbfa4267850ed52996c59e5b"
     }
    },
    "35e204c5e19048a6bfb6409ee98b38d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87395cfdfbfa4267850ed52996c59e5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8757169d3fcb4abb903cf552f14afba7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05be7abbc0c647ee917fd88f37ef974c",
      "placeholder": "​",
      "style": "IPY_MODEL_0a5311d5e8e740ac93f5bd05839fd4e0",
      "value": " 12/12 [00:00&lt;00:00, 624.87it/s]"
     }
    },
    "9c024792d7384dee8a20fbd46f734809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7ef24137af24a699a21a0428283c7a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c95d0f37320a4c7c816b98d7f6d76e4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffdb91d3e58e487c93d9c4b705dfe8af",
      "placeholder": "​",
      "style": "IPY_MODEL_35e204c5e19048a6bfb6409ee98b38d5",
      "value": "Fetching 12 files: 100%"
     }
    },
    "ffdb91d3e58e487c93d9c4b705dfe8af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
